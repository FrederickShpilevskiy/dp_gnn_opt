{"activation_fn": "relu", "batch_size": 5455, "learning_rate": 0.0008849369152171947, "max_degree": 30, "num_decoder_layers": 2, "num_encoder_layers": 1, "num_training_steps": 1549, "optimizer": "adam", "trial_number": 28}